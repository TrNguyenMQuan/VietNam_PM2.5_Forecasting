{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac99dcb0",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING\n",
    "Data preprocessing is the key step in any data analysis or machine learning pipeline. This step often interleaved with **Data Exploration**. It involves cleaning, transforming and organizing raw data to ensure it is accurate, consistent and ready for modeling. It has a big impact on model building such as:\n",
    "\n",
    "- Clean and well-structured data allows models to learn meaningful patterns rather than noise.\n",
    "\n",
    "- Properly processed data prevents misleading inputs, leading to more reliable predictions.\n",
    "\n",
    "- Organized data makes it simpler to create useful inputs for the model, enhancing model performance.\n",
    "\n",
    "- Organized data supports better Exploratory Data Analysis (EDA), making patterns and trends more interpretable.\n",
    "\n",
    "**Steps in Data Preprocessing:** Some key steps in data preprocessing are: **Data Cleaning**, **Data Integration**, **Data Transformation**, and **Feature Engineering**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be24b9",
   "metadata": {},
   "source": [
    "## I. Data Cleaning\n",
    "It is the process of correcting errors or inconsistencies, normalization and handling any missing, duplicated or irrelevant data in the dataset which was discovered by us through step **Data Exploration**. Clean data is essential for effective analysis, as it improves the quality of results and enhances the performance of data models. We will seperate data cleaning into **six key steps**, included in:\n",
    "\n",
    "- **Synstax Errors**: Correcting issues such as **typos, incorrect data types, and invalid characters**.\n",
    "\n",
    "- **Format Data**: Standardizing data formats, including **dates, numeric values, text cases, and units of measurement**.\n",
    "\n",
    "- **Irrelevant Data**: Removing data fields or records that do not contribute to the analytical objective.\n",
    "\n",
    "- **Handling Missing Data**: Addressing missing values through **removal, imputation, or estimation**.\n",
    "\n",
    "- **Handling Duplicated Data**: Eliminating duplicate records.\n",
    "\n",
    "- **Handling Outlier & Impossible Values**: Managing extreme or abnormal values which is detected.\n",
    "\n",
    "Through insights gained from **Data Understanding of the Raw Dataset**, we recognize that the collected data is relatively clean, with no duplicated and wrong format. Therefore, in this section, we only focus on addressing the remaining issues through necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0a9ca",
   "metadata": {},
   "source": [
    "At the beginning like any step, we also need import the libraries as well as load the data by `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9ef47",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ae945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from src import preprocessing as dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09648d12",
   "metadata": {},
   "source": [
    "**Load the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a61d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS]: Loading dataset successful\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/raw/vietnam_air_quality.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(path)\n",
    "    print(\"[SUCCESS]: Loading dataset successful\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR]: Loading dataset fail: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c22be4",
   "metadata": {},
   "source": [
    "### 1. Format Data\n",
    "As we explored in step **explore datatype of the data** at **Data Exploration**. Most of columns have been right datatype except column `timestamp`. Therefore, in this section we will convert it to right datatype that is `datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe4885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Formating data is successful\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='raise')\n",
    "    print(\"[SUCCESS] Formating data is successful\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Formating data fail: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f7ef3e",
   "metadata": {},
   "source": [
    "### 2. Irrelevant Data\n",
    "The goal of this step is reduce memory and noise by removing columns which is not essential for analysis as well as build model.\n",
    "\n",
    "With **20 columns** in our dataset, we will decide to drop column `lat`, `lon`  because the data from the above column was used for mapping to create the **`city`** column.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204cb41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Irrelevant column is successfull\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df.drop(columns=[\"lat\", \"lon\"], inplace=True)\n",
    "    print(\"[SUCCESS] Irrelevant column is successfull\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Irrelevant column is fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011a76b",
   "metadata": {},
   "source": [
    "### 3. Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd61a6e",
   "metadata": {},
   "source": [
    "As identified in the **Data Exploration** phase, **93 records** violated the physical constraint where fine particulate matter ($PM_{2.5}$) exceeded coarse particulate matter ($PM_{10}$). In addition, a small number of records were found where $PM_{2.5}$ or $PM_{10}$ were equal to zero values that are physically implausible in the context of air quality monitoring and are likely indicative of sensor malfunction or implicitly encoded missing data.\n",
    "\n",
    "To address these anomalies, we employed a **four-step imputation strategy**:\n",
    "\n",
    "1. **Zero-value Masking:**  \n",
    "   Records where $PM_{2.5}$ or $PM_{10}$ equaled zero were treated as implicit missing values and replaced with `NaN`.\n",
    "\n",
    "2. **Constraint-based Masking:**  \n",
    "   For records violating the physical constraint $PM_{2.5} > PM_{10}$, the invalid $PM_{10}$ values were converted to `NaN` (treated as missing).\n",
    "\n",
    "3. **Time-based Interpolation:**  \n",
    "   Missing values were reconstructed using **time-based interpolation**. Crucially, this operation was performed **within each city** to preserve spatial independence and local temporal patterns.\n",
    "\n",
    "4. **Constraint Enforcement:**  \n",
    "   A final consistency check was applied to strictly enforce the physical constraint $PM_{10} \\geq PM_{2.5}$ across the entire dataset, ensuring physical validity after imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7472850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect PM2.5 or PM10 equal to zero (implicit missing values)\n",
    "pm_zero_mask = (df[\"pm2_5\"] == 0) | (df[\"pm10\"] == 0)\n",
    "df.loc[pm_zero_mask, [\"pm2_5\", \"pm10\"]] = np.nan\n",
    "\n",
    "mask_invalid = df[df[\"pm2_5\"] > df[\"pm10\"]].index\n",
    "df.loc[mask_invalid, \"pm10\"] = np.nan\n",
    "\n",
    "# Convert to datetime\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "# Set timestamp as Index (Required for time interpolation)\n",
    "df = df.set_index(\"timestamp\")\n",
    "\n",
    "# Interpolate\n",
    "df[\"pm10\"] = df.groupby(\"city\")[\"pm10\"].transform(\n",
    "    lambda group: group.interpolate(method=\"time\", limit=5)\n",
    ")\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index()\n",
    "df[\"pm10\"] = df[[\"pm10\", \"pm2_5\"]].max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de04956",
   "metadata": {},
   "source": [
    "**Checking After Resolve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805e6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-fix Verification: Found 0 impossible rows.\n"
     ]
    }
   ],
   "source": [
    "remaining_invalid = df[df[\"pm2_5\"] > df[\"pm10\"]].index\n",
    "print(f\"Post-fix Verification: Found {len(remaining_invalid)} impossible rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12634e",
   "metadata": {},
   "source": [
    "### 4. Handling Outlier\n",
    "Before jumping into handle outlier we need save processed data for explore continuously which we process in previous step. This data is ready to define about distribution data and from this we can have insight about outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177eb33",
   "metadata": {},
   "source": [
    "#### 4.1 Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/processed/processed_data.csv\"\n",
    "df.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca07dc1",
   "metadata": {},
   "source": [
    "Based on the insights derived from the `data_exploration.ipynb` notebook and the physical characteristics of air quality data, we have decided **NOT to remove or alter statistical outliers** (extreme high values). These are some reason about it:\n",
    "\n",
    "1. **Authentic Environmental Variability vs. Error**\n",
    "   - **Insight:** As noted in the **Data Exploration** phase, the \"outliers\" (e.g., AQI > 150) represent periods of **severe air pollution events**.\n",
    "   - **Reasoning:** In environmental science, extreme values are often \"features,\" not \"bugs.\" A value of `AQI = 146` (as seen in *Bac Ninh*) is a realistic occurrence during winter or temperature inversion periods. Removing these values would artificially smooth the data, causing it to deviate from reality.\n",
    "\n",
    "2. **Preservation of Time-Series Continuity**\n",
    "    - **Constraint:** The data is a continuous hourly time series.\n",
    "    - **Reasoning:** Deep learning models (e.g., LSTM, GRU) and feature engineering techniques (e.g., Lag features $t-1$, Rolling Windows) require an unbroken temporal sequence. Deleting rows containing outliers would create **time gaps**, disrupting the temporal dependencies required for accurate forecasting.\n",
    "\n",
    "3. **Multivariate Integrity**\n",
    "   - **Logic:** Pollution levels are physically correlated with meteorological factors (e.g., high PM2.5 often correlates with low `wind_speed` or high `humidity`).\n",
    "   - **Reasoning:** Removing a high PM2.5 data point while retaining its corresponding weather conditions destroys the physical cause-and-effect relationship. The model needs these extreme examples to learn how specific weather patterns trigger pollution spikes.\n",
    "\n",
    "\n",
    "However, if in step **Modeling** the accurancy or efficiency of model doesn't good performance we will consider to fall back in this step - **Handling Outliers**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f4246",
   "metadata": {},
   "source": [
    "## II. Data Intergation\n",
    "It involves merging data from various sources into a single, unified dataset. It can be challenging due to differences in data formats, structures, and meanings. However, this dataset only store in one file `csv`, so at this step we will not record linkage as well as data fusion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
