{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac99dcb0",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING\n",
    "Data preprocessing is the key step in any data analysis or machine learning pipeline. This step often interleaved with **Data Exploration**. It involves cleaning, transforming and organizing raw data to ensure it is accurate, consistent and ready for modeling. It has a big impact on model building such as:\n",
    "\n",
    "- Clean and well-structured data allows models to learn meaningful patterns rather than noise.\n",
    "\n",
    "- Properly processed data prevents misleading inputs, leading to more reliable predictions.\n",
    "\n",
    "- Organized data makes it simpler to create useful inputs for the model, enhancing model performance.\n",
    "\n",
    "- Organized data supports better Exploratory Data Analysis (EDA), making patterns and trends more interpretable.\n",
    "\n",
    "**Steps in Data Preprocessing:** Some key steps in data preprocessing are: **Data Cleaning**, **Data Integration**, **Data Transformation**, **Data Reduction** and **Feature Engineering**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be24b9",
   "metadata": {},
   "source": [
    "## I. Data Cleaning\n",
    "It is the process of correcting errors or inconsistencies, normalization and handling any missing, duplicated or irrelevant data in the dataset which was discovered by us through step **Data Exploration**. Clean data is essential for effective analysis, as it improves the quality of results and enhances the performance of data models. We will seperate data cleaning into **six key steps**, included in:\n",
    "\n",
    "- **Synstax Errors**: Correcting issues such as **typos, incorrect data types, and invalid characters**.\n",
    "\n",
    "- **Format Data**: Standardizing data formats, including **dates, numeric values, text cases, and units of measurement**.\n",
    "\n",
    "- **Irrelevant Data**: Removing data fields or records that do not contribute to the analytical objective.\n",
    "\n",
    "- **Handling Missing Data**: Addressing missing values through **removal, imputation, or estimation**.\n",
    "\n",
    "- **Handling Duplicated Data**: Eliminating duplicate records.\n",
    "\n",
    "- **Handling Outlier**: Managing extreme or abnormal values which is detected.\n",
    "\n",
    "Through insights gained from **Data Understanding of the Raw Dataset**, we recognize that the collected data is relatively clean, with no missing values or duplicates. Therefore, in this section, we focus on addressing the remaining issues through the **Irrelevant Data** and **Format Data** steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0a9ca",
   "metadata": {},
   "source": [
    "At the beginning like any step, we also need import the libraries as well as load the data by `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9ef47",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ae945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from src import preprocessing as dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09648d12",
   "metadata": {},
   "source": [
    "**Load the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06a61d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS]: Loading dataset successful\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/raw/vietnam_air_quality.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(path)\n",
    "    print(\"[SUCCESS]: Loading dataset successful\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR]: Loading dataset fail: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c22be4",
   "metadata": {},
   "source": [
    "### 1. Format Data\n",
    "As we explored in step **explore datatype of the data** at **Data Exploration**. Most of columns have been right datatype except column `timestamp`. Therefore, in this section we will convert it to right datatype that is `datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adfe4885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Formating data is successful\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='raise')\n",
    "    print(\"[SUCCESS] Formating data is successful\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Formating data fail: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f7ef3e",
   "metadata": {},
   "source": [
    "### 2. Irrelevant Data\n",
    "The goal of this step is reduce memory and noise by removing columns which is not essential for analysis as well as build model.\n",
    "\n",
    "With **20 columns** in our dataset, we will decide to drop column `lat`, `lon`  because the data from the above column was used for mapping to create the **`city`** column.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "204cb41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Irrelevant column is successfull\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df.drop(columns=[\"lat\", \"lon\"], inplace=True)\n",
    "    print(\"[SUCCESS] Irrelevant column is successfull\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Irrelevant column is fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011a76b",
   "metadata": {},
   "source": [
    "### 3. Handling Outlier\n",
    "Before jumping into handle outlier we need save processed data for explore continuously which we process in previous step. This data is ready to define about distribution data and from this we can have insight about outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177eb33",
   "metadata": {},
   "source": [
    "### 4. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "943c84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/processed/processed_data.csv\"\n",
    "df.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca07dc1",
   "metadata": {},
   "source": [
    "After checking Outlier through **Data Exploration**, we will decide not to do handle outlier in **Data Cleaning** process because some these reasons:\n",
    "\n",
    "...\n",
    "\n",
    "However, if in step **Modeling** the accurancy or efficiency of model doesn't good performance we will consider to fall back in this step - **Handling Outliers**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f4246",
   "metadata": {},
   "source": [
    "## II. Data Intergation\n",
    "It involves merging data from various sources into a single, unified dataset. It can be challenging due to differences in data formats, structures, and meanings. However, this dataset only store in one file `csv`, so at this step we will not record linkage as well as data fusion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
